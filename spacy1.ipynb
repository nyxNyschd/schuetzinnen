{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPW8mtfaKZB6FTWq90HONIq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyxNyschd/schuetzinnen/blob/lara/spacy1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1EfMu8utsNz"
      },
      "source": [
        "import spacy\n",
        "# Pandas Dataframes\n",
        "import pandas as pd\n",
        "# Regular expressions\n",
        "import re\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "# Create a blank Tokenizer with just the English vocab\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "# Create a Tokenizer with the default settings for English\n",
        "# including punctuation rules and exceptions\n",
        "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "\n",
        "#Datei die ich nochmal neu extrahiert und hochgeladen habe\n",
        "short_desc_eng = pd.read_csv('new_new_staging_xml_2020.csv', delimiter = ',')"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUpSYtRDuikE"
      },
      "source": [
        "short_desc_eng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrptRhDkuldO"
      },
      "source": [
        "shorty = pd.DataFrame(short_desc_eng)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxf7ll-MumGo"
      },
      "source": [
        "shorty ['SHORT_DESC_ENG']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a64DHsWGuoJ7"
      },
      "source": [
        "shorty['SHORT_DESC_ENG'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb-dEjZ6u64w"
      },
      "source": [
        "#tokenisierung\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(shorty ['SHORT_DESC_ENG'][0])\n",
        "\n",
        "print([tok.text for tok in doc])\n",
        "print('original: {}'.format(shorty ['SHORT_DESC_ENG'][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmc2pxsTxleA"
      },
      "source": [
        "#lemmatisierung\n",
        "#spacy hat keine stemming module, sondern nur lemmatization, \n",
        "#dies it aber nicht schlimm weil:\n",
        "#Stemming algorithm works by cutting the suffix or prefix \n",
        "#from the word. Lemmatization is a more powerful operation as \n",
        "#it takes into consideration the morphological analysis of the word. \n",
        "#Lemmatization returns the lemma, which is the root word of all its \n",
        "#inflection forms. We can say that stemming is a quick and dirty \n",
        "#method of chopping off words to its root form while on the other hand,\n",
        "#lemmatization is an intelligent operation that uses dictionaries \n",
        "#which are created by in-depth linguistic knowledge. \n",
        "#Hence, Lemmatization helps in forming better features.\n",
        "\n",
        "#das ist noch eine andere variante lemmatization anzuwenden\n",
        "#import en_core_web_sm\n",
        "#nlp = en_core_web_sm.load()\n",
        "\n",
        "#doc = nlp(shorty ['SHORT_DESC_ENG'][0])\n",
        "#lemma_word1 = [] \n",
        "#for token in doc:\n",
        "    #lemma_word1.append(token.lemma_)\n",
        "#lemma_word1\n",
        "\n",
        "print([tok.lemma_ for tok in doc])\n",
        "print('original: {}'.format(shorty ['SHORT_DESC_ENG'][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ6ofNywxykk"
      },
      "source": [
        "#keine satzzeichen\n",
        "no_punct = [tok.text for tok in doc if re.match('\\w+', tok.text)]\n",
        "no_punct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTLJ8BxWx2CB"
      },
      "source": [
        "#keine zahlen\n",
        "no_numbers = [tok for tok in no_punct if not re.match('\\d+', tok)]\n",
        "no_numbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V53XSoIkKjRu"
      },
      "source": [
        "#keine zahlen\n",
        "result = ''.join([i for i in shorty['SHORT_DESC_ENG'][0] if not i.isdigit()])\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUDHXOvXKfhW"
      },
      "source": [
        "#keine leerzeichen\n",
        "no_whitespaces = [tok for tok in no_punct if not re.match('\\s+', tok)]\n",
        "no_whitespaces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQeCY6SpKmue"
      },
      "source": [
        "#keine zeichen die angegeben sind . + * ( ) [ ] - $ |\n",
        "no_zeichen = [tok for tok in no_punct if not re.match('\\. \\+ \\* \\( \\) \\[ \\] \\- \\$ \\|', tok)]\n",
        "no_zeichen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs0mGrS4x6Lc"
      },
      "source": [
        "#normalisierung also alles wird kleingeschrieben,\n",
        "#wenn ich mich richtig erinnere wollten wir das nicht nutzen aber \n",
        "#ich habe es trotzdem mal mit aufgeschrieben, für den fall das es doch\n",
        "#was bringt \n",
        "\n",
        "#normalized = [tok.lower() for tok in no_numbers]\n",
        "#normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPN0Ln9pyHSz"
      },
      "source": [
        "#alle Stopwords die es im englischen gibt einmal aufgelistet\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "print(STOP_WORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHd4CB2ryLl0"
      },
      "source": [
        "#stopwords werden entfernt \n",
        "\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = shorty ['SHORT_DESC_ENG'][0]\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[] \n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word) \n",
        "print(token_list)\n",
        "print(filtered_sentence)\n",
        "print(len(token_list))\n",
        "print(len(filtered_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jpyComU0UDl"
      },
      "source": [
        "print(len(all_stopwords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSfqnUHR-MmI"
      },
      "source": [
        "all_stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm8JDTjUyS3p"
      },
      "source": [
        "#hier können alle tokens raus gefiltert werden wenn sie \n",
        "# zum beispiel zahlen, satzzeichen, stoppwörtern etc sind,\n",
        "#hierbei wird immer das lemma benutzt\n",
        "\n",
        "\n",
        "spacy_doc = nlp(shorty ['SHORT_DESC_ENG'][0])\n",
        "\n",
        "\n",
        "cleaned = [tok.lemma_ for tok in spacy_doc if (not tok.is_digit and not tok.is_punct and not tok.is_stop)]\n",
        "cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmGxeSXgyZLt"
      },
      "source": [
        "#weitere verbesserung zum filtern muss noch überarbeitet werden\n",
        "#from spacy.tokens import Token\n",
        "\n",
        "# custom stop word identification\n",
        "#def stop_words_getter(token):\n",
        "  #is_stopword = token.text in STOP_WORDS or \\\n",
        "                #token.lower in STOP_WORDS or \\\n",
        "                #token.lemma_ in STOP_WORDS or \\\n",
        "                #token.lemma_.lower() in STOP_WORDS\n",
        "  #return is_stopword\n",
        "\n",
        "# register new extension\n",
        "#Token.set_extension('is_stop_custom', getter=stop_words_getter)\n",
        "\n",
        "# apply pipeline\n",
        "#doc = nlp(shorty ['SHORT_DESC_ENG'][0])\n",
        "\n",
        "# show result\n",
        "#df = pd.DataFrame([[tok.text, tok.lemma_, tok.is_stop, tok._.is_stop_custom] for tok in doc])\n",
        "#df.columns = ['text', 'lemma', 'is_stop: original', 'is_stop: custom']\n",
        "#df"
      ],
      "execution_count": 147,
      "outputs": []
    }
  ]
}