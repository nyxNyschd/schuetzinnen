{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy2_tokenization_tagging_entities.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZW5kh230gn/f+QRsiriLB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyxNyschd/schuetzinnen/blob/lara/spacy2_tokenization_tagging_entities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c22x4pwQzte5"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "# Pandas Dataframes\n",
        "import pandas as pd\n",
        "# Regular expressions\n",
        "import re\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "# Create a blank Tokenizer with just the English vocab\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "# Create a Tokenizer with the default settings for English\n",
        "# including punctuation rules and exceptions\n",
        "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "\n",
        "#Datei die ich nochmal neu extrahiert und hochgeladen habe\n",
        "short_desc_eng = pd.read_csv('new_new_staging_xml_2020.csv', delimiter = ',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZWIsrFZzwsB"
      },
      "source": [
        "short_desc_eng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "manE_AAezwle"
      },
      "source": [
        "shorty = pd.DataFrame(short_desc_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vAjUZswzz_a"
      },
      "source": [
        "shorty ['SHORT_DESC_ENG']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8KH1UJTz1oa"
      },
      "source": [
        "shorty['SHORT_DESC_ENG'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7IaSN7Gz27Z"
      },
      "source": [
        "#tokenisierung\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(shorty ['SHORT_DESC_ENG'][0])\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5lffFCB0ExZ"
      },
      "source": [
        "#parse and tagging\n",
        "#hier nochmal eine aufschlüsselung der einzelnen textzeilen:\n",
        "\n",
        "#Text: The original word text.\n",
        "#Lemma: The base form of the word.\n",
        "#POS: The simple UPOS part-of-speech tag.\n",
        "#Tag: The detailed part-of-speech tag.\n",
        "#Dep: Syntactic dependency, i.e. the relation between tokens.\n",
        "#Shape: The word shape – capitalization, punctuation, digits.\n",
        "#is alpha: Is the token an alpha character?\n",
        "#is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
        "\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "#doc = nlp(shorty ['SHORT_DESC_ENG'][0])\n",
        "\n",
        "#for token in doc:\n",
        "    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            #token.shape_, token.is_alpha, token.is_stop)\n",
        "  \n",
        "df = pd.DataFrame([[tok.text, tok.lemma_, tok.pos_, tok.tag_, tok.dep_,\n",
        "            tok.shape_, tok.is_alpha, tok.is_stop] for tok in doc])\n",
        "df.columns = ['text', 'lemma', 'pos', 'tag', 'dep', 'shape', 'is_alpha', 'is_stop']\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2Q_KG90JTO"
      },
      "source": [
        "#named entities, spacy kann bereits einige entities zuordnen \n",
        "#man muss das vielleicht noch auf eigene bedürfnisse anpassen\n",
        "#hier eine aufschlüsselung der begriffe im print\n",
        "\n",
        "#Text: The original entity text.\n",
        "#Start: Index of start of entity in the Doc.\n",
        "#End: Index of end of entity in the Doc.\n",
        "#Label: Entity label, i.e. type.\n",
        "\n",
        "#hierbei werden sozusagen entities zugeordnet, zum beispiel money,\n",
        "#ob es eine organisation ist, ob es ein land ist, ich weiß \n",
        "#noch nicht genau welche entitäten spacy zur verfügung stellt\n",
        "#da müssten wir nochmal reinschauen ob wir noch andere brauchen \n",
        "\n",
        "\n",
        "text = (shorty ['SHORT_DESC_ENG'][0])\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "\n",
        "\n",
        "#text = [[tok in ]]\n",
        "#df = pd.DataFrame(text, columns = ['text', 'start_char'])\n",
        "\n",
        "\n",
        "#df = pd.DataFrame([[entity.text] for tok in doc])\n",
        "#df.columns = ['text']\n",
        "#df\n",
        "\n",
        "#df = pd.DataFrame(person_counts, columns =['text', 'count'])\n",
        "\n",
        "#hieraus kann ebenfalls eine tabelle geformt werden\n",
        "\n",
        "\n",
        "#def extract_named_ents(shorty):\n",
        "    #return [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in nlp(shorty).ents]\n",
        "\n",
        "#def add_named_ents(df):\n",
        "    \n",
        "    #df['named_ents'] = df['text'].apply(extract_named_ents)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}